{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1bCpOdEEYn//K1kZCESMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImenMasmoudiEm/CCKInternshipProject/blob/main/Ensembling_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mIp4l4Y0_79",
        "outputId": "d5500681-9688-41ba-c47a-6758e69bcccc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H85ybNJ_yLfe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import os \n",
        "os.chdir('/content/drive/MyDrive/All/Projects/Ing Internship/Data')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading Data\n"
      ],
      "metadata": {
        "id": "klpWdEse1MPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tunnel=pd.read_excel(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/final-dataset.xlsx\")\n",
        "Tun=pd.read_excel(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/dataset1and2.xlsx\")\n",
        "Lib=pd.read_excel(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/dataset3and4.xlsx\")\n",
        "Egy=pd.read_excel(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/D5.xlsx\")"
      ],
      "metadata": {
        "id": "kjdUVLTX1Oog"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For the Tunnal Dataset\n",
        "Tunnel['classe']=Tunnel['classe'].replace(\"hate\", int(2))\n",
        "Tunnel['classe']=Tunnel['classe'].replace(\"normal\", int(0)) \n",
        "Tunnel['classe']=Tunnel['classe'].replace(\"abusive\", int(1))\n",
        "\n",
        "\n",
        "TunnelS = Tunnel['commentaire']\n",
        "TunnelL = Tunnel['classe']\n",
        "\n",
        "TunnalL = [int(i) for i in TunnelL]\n",
        "\n",
        "#For the Tunisian Dataset\n",
        "Tun['classe']=Tun['classe'].replace(\"hate\", int(2))\n",
        "Tun['classe']=Tun['classe'].replace(\"normal\", int(0)) \n",
        "Tun['classe']=Tun['classe'].replace(\"abusive\", int(1))\n",
        "\n",
        "\n",
        "TunS = Tun['commentaire']\n",
        "TunL = Tun['classe']\n",
        "\n",
        "TunL = [int(i) for i in TunL]\n",
        "\n",
        "#For the Libanon Dataset\n",
        "Lib['classe']=Lib['classe'].replace(\"hate\", int(2))\n",
        "Lib['classe']=Lib['classe'].replace(\"normal\", int(0)) \n",
        "Lib['classe']=Lib['classe'].replace(\"abusive\", int(1))\n",
        "\n",
        "\n",
        "LibS = Lib['commentaire']\n",
        "LibL = Lib['classe']\n",
        "\n",
        "LibL = [int(i) for i in LibL]\n",
        "\n",
        "#For the Egyptien Dataset\n",
        "Egy['classe']=Egy['classe'].replace(\"hate\", int(2))\n",
        "Egy['classe']=Egy['classe'].replace(\"normal\", int(0)) \n",
        "Egy['classe']=Egy['classe'].replace(\"abusive\", int(1))\n",
        "\n",
        "\n",
        "EgyS = Egy['commentaire']\n",
        "EgyL = Egy['classe']\n",
        "\n",
        "EgyL = [int(i) for i in EgyL]"
      ],
      "metadata": {
        "id": "Cn6s5VUy1pCa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_size=int(len(TunnelS)*0.9) \n",
        "\n",
        "training_sentences = TunnelS[0:training_size]\n",
        "TunnelS = TunnelS[training_size:]\n",
        "TunnelL = TunnelL[training_size:]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=3000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "#For Tunnel Dataset\n",
        "TunnelS = tokenizer.texts_to_sequences(TunnelS)\n",
        "TunnelS207 = pad_sequences(TunnelS, maxlen=207, padding='post', truncating='post')\n",
        "TunnelS388 = pad_sequences(TunnelS, maxlen=388, padding='post', truncating='post')\n",
        "\n",
        "#For Tunisian Dataset\n",
        "training_size=int(len(TunS)*0.8) \n",
        "\n",
        "TunS = TunS[training_size:]\n",
        "TunL = TunL[training_size:]\n",
        "\n",
        "TunS = tokenizer.texts_to_sequences(TunS)\n",
        "TunS207 = pad_sequences(TunS, maxlen=207, padding='post', truncating='post')\n",
        "TunS388 = pad_sequences(TunS, maxlen=388, padding='post', truncating='post')\n",
        "\n",
        "#For Libanon Dataset\n",
        "training_size=int(len(LibS)*0.8) \n",
        "\n",
        "LibS = LibS[training_size:]\n",
        "LibL = LibL[training_size:]\n",
        "\n",
        "LibS = tokenizer.texts_to_sequences(LibS)\n",
        "LibS207 = pad_sequences(LibS, maxlen=207, padding='post', truncating='post')\n",
        "LibS388 = pad_sequences(LibS, maxlen=388, padding='post', truncating='post')\n",
        "\n",
        "#For Egyptian Dataset\n",
        "training_size=int(len(EgyS)*0.8)\n",
        "\n",
        "EgyS = EgyS[training_size:]\n",
        "EgyL = EgyL[training_size:]\n",
        "\n",
        "EgyS = tokenizer.texts_to_sequences(EgyS)\n",
        "EgyS207 = pad_sequences(EgyS, maxlen=207, padding='post', truncating='post')\n",
        "EgyS388 = pad_sequences(EgyS, maxlen=388, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "JWuvFt7Y2g2L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the models\n",
        "M1B09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Balanced 09/Model1Balanced09.h5\")\n",
        "M1U09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Unbalanced 09/Model1Unbalanced09.h5\")\n",
        "M2B09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Balanced 09/Model2Balanced09.h5\")\n",
        "M2U09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Unbalanced 09/Model2Unbalanced09.h5\")\n",
        "M3B09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Balanced 09/Model3Balanced09.h5\")\n",
        "M3U09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Unbalanced 09/Model3Unbalanced09.h5\")"
      ],
      "metadata": {
        "id": "W3gFxkmB5pou"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions for Tunnel\n",
        "TunnelP11=M1B09.predict(TunnelS207)\n",
        "TunnelP21=M1U09.predict(TunnelS388)\n",
        "TunnelP31=M2B09.predict(TunnelS207)\n",
        "TunnelP41=M2U09.predict(TunnelS388)\n",
        "TunnelP51=M3B09.predict(TunnelS207)\n",
        "TunnelP61=M3U09.predict(TunnelS388)\n",
        "\n",
        "#Predictions for Tun\n",
        "TunP11=M1B09.predict(TunS207)\n",
        "TunP21=M1U09.predict(TunS388)\n",
        "TunP31=M2B09.predict(TunS207)\n",
        "TunP41=M2U09.predict(TunS388)\n",
        "TunP51=M3B09.predict(TunS207)\n",
        "TunP61=M3U09.predict(TunS388)\n",
        "\n",
        "#Predictions for Lib\n",
        "LibP11=M1B09.predict(LibS207)\n",
        "LibP21=M1U09.predict(LibS388)\n",
        "LibP31=M2B09.predict(LibS207)\n",
        "LibP41=M2U09.predict(LibS388)\n",
        "LibP51=M3B09.predict(LibS207)\n",
        "LibP61=M3U09.predict(LibS388)\n",
        "\n",
        "#Predictions for Egy\n",
        "EgyP11=M1B09.predict(EgyS207)\n",
        "EgyP21=M1U09.predict(EgyS388)\n",
        "EgyP31=M2B09.predict(EgyS207)\n",
        "EgyP41=M2U09.predict(EgyS388)\n",
        "EgyP51=M3B09.predict(EgyS207)\n",
        "EgyP61=M3U09.predict(EgyS388)"
      ],
      "metadata": {
        "id": "1hU12_YP3aUi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First approach: Argmax and then look for the max\n",
        "\n",
        "def fcount(c,L):\n",
        "  r=0\n",
        "  for i in L:\n",
        "    if i==c:\n",
        "      r+=1\n",
        "  return r\n",
        "\n",
        "def MaxV(L1,L2,L3,L4,L5,L6):\n",
        "  L=np.zeros(len(L1))\n",
        "  for i in range(len(L1)):\n",
        "    if (fcount(2,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])>fcount(0,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])) and (fcount(2,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])>fcount(1,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])):\n",
        "      L[i]=2\n",
        "    elif (fcount(1,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])>fcount(0,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])):\n",
        "      L[i]=1\n",
        "    else:\n",
        "      L[i]=0\n",
        "  return L\n",
        "\n",
        "#For the Tunnel Dataset\n",
        "TunnelP1=np.argmax(TunnelP11, axis=1).astype(int)\n",
        "TunnelP2=np.argmax(TunnelP21, axis=1).astype(int)\n",
        "TunnelP3=np.argmax(TunnelP31, axis=1).astype(int)\n",
        "TunnelP4=np.argmax(TunnelP41, axis=1).astype(int)\n",
        "TunnelP5=np.argmax(TunnelP51, axis=1).astype(int)\n",
        "TunnelP6=np.argmax(TunnelP61, axis=1).astype(int)\n",
        "\n",
        "TunnelP=MaxV(TunnelP1,TunnelP2,TunnelP3,TunnelP4,TunnelP5,TunnelP6)\n",
        "\n",
        "cm = confusion_matrix(TunnelL, TunnelP)\n",
        "print('Testing on the Tunnel Dataset')\n",
        "print(cm)\n",
        "print(classification_report(TunnelL, TunnelP, labels=[0,1,2]))\n",
        "\n",
        "#For the Tunisian Dataset\n",
        "TunP1=np.argmax(TunP11, axis=1).astype(int)\n",
        "TunP2=np.argmax(TunP21, axis=1).astype(int)\n",
        "TunP3=np.argmax(TunP31, axis=1).astype(int)\n",
        "TunP4=np.argmax(TunP41, axis=1).astype(int)\n",
        "TunP5=np.argmax(TunP51, axis=1).astype(int)\n",
        "TunP6=np.argmax(TunP61, axis=1).astype(int)\n",
        "\n",
        "TunP=MaxV(TunP1,TunP2,TunP3,TunP4,TunP5,TunP6)\n",
        "\n",
        "cm = confusion_matrix(TunL, TunP)\n",
        "print('Testing on the Tunisian Dataset')\n",
        "print(cm)\n",
        "print(classification_report(TunL, TunP, labels=[0,1,2]))\n",
        "\n",
        "#For the Libanon Dataset\n",
        "LibP1=np.argmax(LibP11, axis=1).astype(int)\n",
        "LibP2=np.argmax(LibP21, axis=1).astype(int)\n",
        "LibP3=np.argmax(LibP31, axis=1).astype(int)\n",
        "LibP4=np.argmax(LibP41, axis=1).astype(int)\n",
        "LibP5=np.argmax(LibP51, axis=1).astype(int)\n",
        "LibP6=np.argmax(LibP61, axis=1).astype(int)\n",
        "\n",
        "LibP=MaxV(LibP1,LibP2,LibP3,LibP4,LibP5,LibP6)\n",
        "\n",
        "cm = confusion_matrix(LibL, LibP)\n",
        "print('Testing on the Libanon Dataset')\n",
        "print(cm)\n",
        "print(classification_report(LibL, LibP, labels=[0,1,2]))\n",
        "\n",
        "#For the Egyptian Dataset\n",
        "EgyP1=np.argmax(EgyP11, axis=1).astype(int)\n",
        "EgyP2=np.argmax(EgyP21, axis=1).astype(int)\n",
        "EgyP3=np.argmax(EgyP31, axis=1).astype(int)\n",
        "EgyP4=np.argmax(EgyP41, axis=1).astype(int)\n",
        "EgyP5=np.argmax(EgyP51, axis=1).astype(int)\n",
        "EgyP6=np.argmax(EgyP61, axis=1).astype(int)\n",
        "\n",
        "EgyP=MaxV(EgyP1,EgyP2,EgyP3,EgyP4,EgyP5,EgyP6)\n",
        "\n",
        "cm = confusion_matrix(EgyL, EgyP)\n",
        "print('Testing on the Egyptian Dataset')\n",
        "print(cm)\n",
        "print(classification_report(EgyL, EgyP, labels=[0,1,2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b81uhYBIeDUo",
        "outputId": "600c9750-ed5c-4237-c1b8-6693bb7779a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on the Tunnel Dataset\n",
            "[[1256    3    6]\n",
            " [ 135  268   11]\n",
            " [ 187  100  338]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.99      0.88      1265\n",
            "           1       0.72      0.65      0.68       414\n",
            "           2       0.95      0.54      0.69       625\n",
            "\n",
            "    accuracy                           0.81      2304\n",
            "   macro avg       0.82      0.73      0.75      2304\n",
            "weighted avg       0.83      0.81      0.79      2304\n",
            "\n",
            "Testing on the Tunisian Dataset\n",
            "[[516  44  27]\n",
            " [118 126   9]\n",
            " [176 131 145]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.88      0.74       587\n",
            "           1       0.42      0.50      0.45       253\n",
            "           2       0.80      0.32      0.46       452\n",
            "\n",
            "    accuracy                           0.61      1292\n",
            "   macro avg       0.62      0.57      0.55      1292\n",
            "weighted avg       0.65      0.61      0.58      1292\n",
            "\n",
            "Testing on the Libanon Dataset\n",
            "[[511   3  12]\n",
            " [  1   5   1]\n",
            " [356 182 769]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.97      0.73       526\n",
            "           1       0.03      0.71      0.05         7\n",
            "           2       0.98      0.59      0.74      1307\n",
            "\n",
            "    accuracy                           0.70      1840\n",
            "   macro avg       0.53      0.76      0.51      1840\n",
            "weighted avg       0.87      0.70      0.73      1840\n",
            "\n",
            "Testing on the Egyptian Dataset\n",
            "[[96  0  0]\n",
            " [ 9 33  0]\n",
            " [15 15 52]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        96\n",
            "           1       0.69      0.79      0.73        42\n",
            "           2       1.00      0.63      0.78        82\n",
            "\n",
            "    accuracy                           0.82       220\n",
            "   macro avg       0.83      0.81      0.80       220\n",
            "weighted avg       0.85      0.82      0.82       220\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Second approach: We add the predictions and then we apply Argmax\n",
        "\n",
        "#Testing on the Tunnel Dataset\n",
        "TunnelPF=np.asarray(TunnelP11)+np.asarray(TunnelP21)+np.asarray(TunnelP31)+np.asarray(TunnelP41)+np.asarray(TunnelP51)+np.asarray(TunnelP61)\n",
        "TunnelPF=np.argmax(TunnelPF, axis=1).astype(int)\n",
        "\n",
        "cm = confusion_matrix(TunnelL, TunnelPF)\n",
        "print('Testing on the Tunnel Dataset')\n",
        "print(cm)\n",
        "print(classification_report(TunnelL, TunnelPF, labels=[0,1,2]))\n",
        "\n",
        "#Testing on the Tunisian Dataset\n",
        "TunPF=np.asarray(TunP11)+np.asarray(TunP21)+np.asarray(TunP31)+np.asarray(TunP41)+np.asarray(TunP51)+np.asarray(TunP61)\n",
        "TunPF=np.argmax(TunPF, axis=1).astype(int)\n",
        "\n",
        "cm = confusion_matrix(TunL, TunPF)\n",
        "print('Testing on the Tunisian Dataset')\n",
        "print(cm)\n",
        "print(classification_report(TunL, TunPF, labels=[0,1,2]))\n",
        "\n",
        "#Testing on the Libanon Dataset\n",
        "LibPF=np.asarray(LibP11)+np.asarray(LibP21)+np.asarray(LibP31)+np.asarray(LibP41)+np.asarray(LibP51)+np.asarray(LibP61)\n",
        "LibPF=np.argmax(LibPF, axis=1).astype(int)\n",
        "\n",
        "cm = confusion_matrix(LibL, LibPF)\n",
        "print('Testing on the Libanon Dataset')\n",
        "print(cm)\n",
        "print(classification_report(LibL, LibPF, labels=[0,1,2]))\n",
        "\n",
        "#Testing on the Egyptian Dataset\n",
        "EgyPF=np.asarray(EgyP11)+np.asarray(EgyP21)+np.asarray(EgyP31)+np.asarray(EgyP41)+np.asarray(EgyP51)+np.asarray(EgyP61)\n",
        "EgyPF=np.argmax(EgyPF, axis=1).astype(int)\n",
        "\n",
        "cm = confusion_matrix(EgyL, EgyPF)\n",
        "print('Testing on the Egyptian Dataset')\n",
        "print(cm)\n",
        "print(classification_report(EgyL, EgyPF, labels=[0,1,2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_olf9YI-zL4",
        "outputId": "3f642ab4-0aee-44ef-aaa8-e546f1965ffe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on the Tunnel Dataset\n",
            "[[1157   41   67]\n",
            " [  63  292   59]\n",
            " [  90   50  485]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.91      0.90      1265\n",
            "           1       0.76      0.71      0.73       414\n",
            "           2       0.79      0.78      0.78       625\n",
            "\n",
            "    accuracy                           0.84      2304\n",
            "   macro avg       0.81      0.80      0.81      2304\n",
            "weighted avg       0.84      0.84      0.84      2304\n",
            "\n",
            "Testing on the Tunisian Dataset\n",
            "[[464  64  59]\n",
            " [ 53 175  25]\n",
            " [ 96 144 212]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.79      0.77       587\n",
            "           1       0.46      0.69      0.55       253\n",
            "           2       0.72      0.47      0.57       452\n",
            "\n",
            "    accuracy                           0.66      1292\n",
            "   macro avg       0.64      0.65      0.63      1292\n",
            "weighted avg       0.68      0.66      0.66      1292\n",
            "\n",
            "Testing on the Libanon Dataset\n",
            "[[ 415   37   74]\n",
            " [   0    3    4]\n",
            " [ 136   58 1113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.79      0.77       526\n",
            "           1       0.03      0.43      0.06         7\n",
            "           2       0.93      0.85      0.89      1307\n",
            "\n",
            "    accuracy                           0.83      1840\n",
            "   macro avg       0.57      0.69      0.57      1840\n",
            "weighted avg       0.88      0.83      0.85      1840\n",
            "\n",
            "Testing on the Egyptian Dataset\n",
            "[[90  1  5]\n",
            " [ 6 29  7]\n",
            " [ 7  4 71]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.94      0.90        96\n",
            "           1       0.85      0.69      0.76        42\n",
            "           2       0.86      0.87      0.86        82\n",
            "\n",
            "    accuracy                           0.86       220\n",
            "   macro avg       0.86      0.83      0.84       220\n",
            "weighted avg       0.86      0.86      0.86       220\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Third approach: We use a Neural Network Model\n",
        "\n",
        "#The Tunnel Dataset\n",
        "TunnelL=np.asarray(TunnelL)\n",
        "TunnelP=np.asarray(TunnelP)\n",
        "XTunnel=[]\n",
        "for i in range(len(TunnelL)):\n",
        "  #print([TunnelP1[i],TunnelP2[i],TunnelP3[i],TunnelP4[i],TunnelP5[i],TunnelP6[i]],\" the true value is: \", TunnelL[i], \"P= \", TunnelP[i])\n",
        "  XTunnel.append([TunnelP1[i],TunnelP2[i],TunnelP3[i],TunnelP4[i],TunnelP5[i],TunnelP6[i]])\n",
        "\n",
        "#The Tunisian Dataset\n",
        "TunL=np.asarray(TunL)\n",
        "TunP=np.asarray(TunP)\n",
        "XTun=[]\n",
        "for i in range(len(TunL)):\n",
        "  XTun.append([TunP1[i],TunP2[i],TunP3[i],TunP4[i],TunP5[i],TunP6[i]])\n",
        "\n",
        "#The Libanon Dataset\n",
        "LibL=np.asarray(LibL)\n",
        "LibP=np.asarray(LibP)\n",
        "XLib=[]\n",
        "for i in range(len(LibL)):\n",
        "  XLib.append([LibP1[i],LibP2[i],LibP3[i],LibP4[i],LibP5[i],LibP6[i]])\n",
        "\n",
        "#The Egyptian Dataset\n",
        "EgyL=np.asarray(EgyL)\n",
        "EgyP=np.asarray(EgyP)\n",
        "XEgy=[]\n",
        "for i in range(len(EgyL)):\n",
        "  XEgy.append([EgyP1[i],EgyP2[i],EgyP3[i],EgyP4[i],EgyP5[i],EgyP6[i]])"
      ],
      "metadata": {
        "id": "cOIfAkZzjXIQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TunnelL=np.asarray(TunnelL)\n",
        "TunL=np.asarray(TunL)\n",
        "EgyL=np.asarray(EgyL)\n",
        "LibL=np.asarray(LibL)\n",
        "\n",
        "XTunnel=np.asarray(XTunnel)\n",
        "XTun=np.asarray(XTun)\n",
        "XEgy=np.asarray(XEgy)\n",
        "XLib=np.asarray(XLib)"
      ],
      "metadata": {
        "id": "hwlNLFSVMwW0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEModelTunnel = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(6,)),\n",
        "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(24, activation='selu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "FEModelTunnel.summary()\n",
        "FEModelTun = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(6,)),\n",
        "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(24, activation='selu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "FEModelLib = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(6,)),\n",
        "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(24, activation='selu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "FEModelEgy = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(6,)),\n",
        "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(24, activation='selu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EFktnE7km3P",
        "outputId": "341ca70a-d1af-4f04-ea04-301c8d353fd4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 6)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               896       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                3096      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 75        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,067\n",
            "Trainable params: 4,067\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FEModelTunnel.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
        "history = FEModelTunnel.fit(XTunnel, TunnelL, epochs=50, verbose=1)\n",
        "\n",
        "FEModelTun.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
        "history = FEModelTun.fit(XTun, TunL, epochs=50, verbose=1)\n",
        "\n",
        "FEModelLib.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
        "history = FEModelLib.fit(XLib, LibL, epochs=50, verbose=1)\n",
        "\n",
        "FEModelEgy.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
        "history = FEModelEgy.fit(XEgy, EgyL, epochs=50, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adwrCC1Ok853",
        "outputId": "8408c499-439d-4217-f97e-4df787ee1166"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "72/72 [==============================] - 1s 2ms/step - loss: 0.7016 - accuracy: 0.7292\n",
            "Epoch 2/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8898\n",
            "Epoch 3/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.3240 - accuracy: 0.9588\n",
            "Epoch 4/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.9588\n",
            "Epoch 5/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.9588\n",
            "Epoch 6/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2920 - accuracy: 0.9588\n",
            "Epoch 7/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2917 - accuracy: 0.9588\n",
            "Epoch 8/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2823 - accuracy: 0.9588\n",
            "Epoch 9/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.9588\n",
            "Epoch 10/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2703 - accuracy: 0.9588\n",
            "Epoch 11/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.9588\n",
            "Epoch 12/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2569 - accuracy: 0.9588\n",
            "Epoch 13/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9588\n",
            "Epoch 14/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.9588\n",
            "Epoch 15/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2354 - accuracy: 0.9588\n",
            "Epoch 16/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9588\n",
            "Epoch 17/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2213 - accuracy: 0.9588\n",
            "Epoch 18/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2165 - accuracy: 0.9588\n",
            "Epoch 19/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2115 - accuracy: 0.9588\n",
            "Epoch 20/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.9588\n",
            "Epoch 21/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2025 - accuracy: 0.9588\n",
            "Epoch 22/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2118 - accuracy: 0.9588\n",
            "Epoch 23/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2061 - accuracy: 0.9588\n",
            "Epoch 24/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2035 - accuracy: 0.9588\n",
            "Epoch 25/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2038 - accuracy: 0.9588\n",
            "Epoch 26/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9588\n",
            "Epoch 27/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2003 - accuracy: 0.9588\n",
            "Epoch 28/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2019 - accuracy: 0.9588\n",
            "Epoch 29/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2016 - accuracy: 0.9588\n",
            "Epoch 30/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2016 - accuracy: 0.9588\n",
            "Epoch 31/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9588\n",
            "Epoch 32/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9588\n",
            "Epoch 33/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9588\n",
            "Epoch 34/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9588\n",
            "Epoch 35/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9588\n",
            "Epoch 36/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9588\n",
            "Epoch 37/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9588\n",
            "Epoch 38/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2009 - accuracy: 0.9588\n",
            "Epoch 39/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9588\n",
            "Epoch 40/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9588\n",
            "Epoch 41/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1984 - accuracy: 0.9588\n",
            "Epoch 42/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9588\n",
            "Epoch 43/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9588\n",
            "Epoch 44/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9588\n",
            "Epoch 45/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1983 - accuracy: 0.9588\n",
            "Epoch 46/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9588\n",
            "Epoch 47/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2022 - accuracy: 0.9588\n",
            "Epoch 48/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9588\n",
            "Epoch 49/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1985 - accuracy: 0.9588\n",
            "Epoch 50/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1972 - accuracy: 0.9588\n",
            "Epoch 1/50\n",
            "41/41 [==============================] - 1s 2ms/step - loss: 1.0846 - accuracy: 0.5008\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.9064 - accuracy: 0.6509\n",
            "Epoch 3/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8515 - accuracy: 0.6571\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8234 - accuracy: 0.6741\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8230 - accuracy: 0.6649\n",
            "Epoch 6/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8182 - accuracy: 0.6780\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.8192 - accuracy: 0.6687\n",
            "Epoch 8/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8099 - accuracy: 0.6765\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8133 - accuracy: 0.6726\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8094 - accuracy: 0.6858\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8069 - accuracy: 0.6796\n",
            "Epoch 12/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8112 - accuracy: 0.6734\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7953 - accuracy: 0.6834\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7927 - accuracy: 0.6803\n",
            "Epoch 15/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7966 - accuracy: 0.6873\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7952 - accuracy: 0.6989\n",
            "Epoch 17/50\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7950 - accuracy: 0.6981\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8092 - accuracy: 0.6958\n",
            "Epoch 19/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7823 - accuracy: 0.6765\n",
            "Epoch 20/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7742 - accuracy: 0.6966\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7744 - accuracy: 0.6950\n",
            "Epoch 22/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7774 - accuracy: 0.6974\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7714 - accuracy: 0.6834\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7628 - accuracy: 0.6873\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7597 - accuracy: 0.7012\n",
            "Epoch 26/50\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7571 - accuracy: 0.6997\n",
            "Epoch 27/50\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7527 - accuracy: 0.7051\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7580 - accuracy: 0.7036\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7546 - accuracy: 0.7067\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7492 - accuracy: 0.6935\n",
            "Epoch 31/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7484 - accuracy: 0.7005\n",
            "Epoch 32/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7525 - accuracy: 0.6920\n",
            "Epoch 33/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7402 - accuracy: 0.7098\n",
            "Epoch 34/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7402 - accuracy: 0.7036\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7368 - accuracy: 0.7098\n",
            "Epoch 36/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7427 - accuracy: 0.7051\n",
            "Epoch 37/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7367 - accuracy: 0.7059\n",
            "Epoch 38/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7345 - accuracy: 0.7098\n",
            "Epoch 39/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7362 - accuracy: 0.7152\n",
            "Epoch 40/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7370 - accuracy: 0.7105\n",
            "Epoch 41/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7358 - accuracy: 0.7036\n",
            "Epoch 42/50\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7383 - accuracy: 0.7175\n",
            "Epoch 43/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7369 - accuracy: 0.7167\n",
            "Epoch 44/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7386 - accuracy: 0.7082\n",
            "Epoch 45/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7314 - accuracy: 0.7121\n",
            "Epoch 46/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7301 - accuracy: 0.7175\n",
            "Epoch 47/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7324 - accuracy: 0.7152\n",
            "Epoch 48/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7290 - accuracy: 0.7183\n",
            "Epoch 49/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7294 - accuracy: 0.7082\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7353 - accuracy: 0.7121\n",
            "Epoch 1/50\n",
            "58/58 [==============================] - 1s 2ms/step - loss: 0.4575 - accuracy: 0.8402\n",
            "Epoch 2/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1635 - accuracy: 0.9793\n",
            "Epoch 3/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1175 - accuracy: 0.9793\n",
            "Epoch 4/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1112 - accuracy: 0.9793\n",
            "Epoch 5/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1088 - accuracy: 0.9793\n",
            "Epoch 6/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1084 - accuracy: 0.9793\n",
            "Epoch 7/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1073 - accuracy: 0.9783\n",
            "Epoch 8/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1088 - accuracy: 0.9788\n",
            "Epoch 9/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.9793\n",
            "Epoch 10/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9788\n",
            "Epoch 11/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9793\n",
            "Epoch 12/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1091 - accuracy: 0.9788\n",
            "Epoch 13/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.9793\n",
            "Epoch 14/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 0.9788\n",
            "Epoch 15/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1095 - accuracy: 0.9788\n",
            "Epoch 16/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1069 - accuracy: 0.9793\n",
            "Epoch 17/50\n",
            "58/58 [==============================] - 0s 3ms/step - loss: 0.1106 - accuracy: 0.9788\n",
            "Epoch 18/50\n",
            "58/58 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.9793\n",
            "Epoch 19/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1095 - accuracy: 0.9783\n",
            "Epoch 20/50\n",
            "58/58 [==============================] - 0s 3ms/step - loss: 0.1091 - accuracy: 0.9788\n",
            "Epoch 21/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.9793\n",
            "Epoch 22/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1099 - accuracy: 0.9793\n",
            "Epoch 23/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9788\n",
            "Epoch 24/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1103 - accuracy: 0.9788\n",
            "Epoch 25/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1058 - accuracy: 0.9793\n",
            "Epoch 26/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1094 - accuracy: 0.9783\n",
            "Epoch 27/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1067 - accuracy: 0.9793\n",
            "Epoch 28/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1072 - accuracy: 0.9788\n",
            "Epoch 29/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1067 - accuracy: 0.9793\n",
            "Epoch 30/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1081 - accuracy: 0.9793\n",
            "Epoch 31/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1120 - accuracy: 0.9788\n",
            "Epoch 32/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1071 - accuracy: 0.9793\n",
            "Epoch 33/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1072 - accuracy: 0.9788\n",
            "Epoch 34/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9788\n",
            "Epoch 35/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1057 - accuracy: 0.9793\n",
            "Epoch 36/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1085 - accuracy: 0.9793\n",
            "Epoch 37/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1072 - accuracy: 0.9793\n",
            "Epoch 38/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1074 - accuracy: 0.9793\n",
            "Epoch 39/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1067 - accuracy: 0.9793\n",
            "Epoch 40/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9793\n",
            "Epoch 41/50\n",
            "58/58 [==============================] - 0s 3ms/step - loss: 0.1072 - accuracy: 0.9793\n",
            "Epoch 42/50\n",
            "58/58 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.9793\n",
            "Epoch 43/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1069 - accuracy: 0.9793\n",
            "Epoch 44/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1089 - accuracy: 0.9783\n",
            "Epoch 45/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1080 - accuracy: 0.9788\n",
            "Epoch 46/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1072 - accuracy: 0.9788\n",
            "Epoch 47/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9793\n",
            "Epoch 48/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1060 - accuracy: 0.9793\n",
            "Epoch 49/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1064 - accuracy: 0.9788\n",
            "Epoch 50/50\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.1062 - accuracy: 0.9793\n",
            "Epoch 1/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.9971 - accuracy: 0.4955\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.9100 - accuracy: 0.6682\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.8094 - accuracy: 0.8045\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.7301 - accuracy: 0.8045\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.8045\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.8045\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.8045\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5153 - accuracy: 0.8045\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4812 - accuracy: 0.8045\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4425 - accuracy: 0.8045\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4158 - accuracy: 0.8364\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3880 - accuracy: 0.8045\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3701 - accuracy: 0.9591\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3498 - accuracy: 0.8318\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3290 - accuracy: 0.9773\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3037 - accuracy: 0.9955\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8591\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2867 - accuracy: 0.9955\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2598 - accuracy: 0.9955\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2498 - accuracy: 0.9955\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.2362 - accuracy: 0.9955\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2211 - accuracy: 0.9955\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9955\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1997 - accuracy: 0.9955\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1909 - accuracy: 0.9955\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1811 - accuracy: 0.9955\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 0.9955\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9955\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1587 - accuracy: 0.9955\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1513 - accuracy: 0.9955\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.9955\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1395 - accuracy: 0.9955\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9955\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.9955\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1255 - accuracy: 0.9955\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1219 - accuracy: 0.9955\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1178 - accuracy: 0.9955\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1140 - accuracy: 0.9955\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1110 - accuracy: 0.9955\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.9955\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1061 - accuracy: 0.9955\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9955\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1007 - accuracy: 0.9955\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0989 - accuracy: 0.9955\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9955\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9955\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.9955\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9955\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0911 - accuracy: 0.9955\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0889 - accuracy: 0.9955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TunnelFP=FEModelTunnel.predict(XTunnel)\n",
        "TunFP=FEModelTun.predict(XTun)\n",
        "EgyFP=FEModelEgy.predict(XEgy)\n",
        "LibFP=FEModelLib.predict(XLib)"
      ],
      "metadata": {
        "id": "MRnFhH1YOPz2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Third approach on the Tunnel Dataset\n",
        "TunnelFP=np.argmax(TunnelFP, axis=1).astype(int)\n",
        "cm = confusion_matrix(TunnelL, TunnelFP)\n",
        "print('Testing on the Tunnel Dataset')\n",
        "print(cm)\n",
        "print(classification_report(TunnelL, TunnelFP, labels=[0,1,2]))\n",
        "\n",
        "#Testing Third approach on the Tunisian Dataset\n",
        "TunFP=np.argmax(TunFP, axis=1).astype(int)\n",
        "cm = confusion_matrix(TunL, TunFP)\n",
        "print('Testing on the Tunisian Dataset')\n",
        "print(cm)\n",
        "print(classification_report(TunL, TunFP, labels=[0,1,2]))\n",
        "\n",
        "#Testing Third approach on the Libanon Dataset\n",
        "LibFP=np.argmax(LibFP, axis=1).astype(int)\n",
        "cm = confusion_matrix(LibL, LibFP)\n",
        "print('Testing on the Libanon Dataset')\n",
        "print(cm)\n",
        "print(classification_report(LibL, LibFP, labels=[0,1,2]))\n",
        "\n",
        "#Testing Third approach on the Egyptian Dataset\n",
        "EgyFP=np.argmax(EgyFP, axis=1).astype(int)\n",
        "cm = confusion_matrix(EgyL, EgyFP)\n",
        "print('Testing on the Egyptian Dataset')\n",
        "print(cm)\n",
        "print(classification_report(EgyL, EgyFP, labels=[0,1,2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sp_G1OwOej9",
        "outputId": "24c6dd0c-6be6-4ec4-f867-18a73bba08d0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on the Tunnel Dataset\n",
            "[[1254    2    9]\n",
            " [  15  375   24]\n",
            " [  33   12  580]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98      1265\n",
            "           1       0.96      0.91      0.93       414\n",
            "           2       0.95      0.93      0.94       625\n",
            "\n",
            "    accuracy                           0.96      2304\n",
            "   macro avg       0.96      0.94      0.95      2304\n",
            "weighted avg       0.96      0.96      0.96      2304\n",
            "\n",
            "Testing on the Tunisian Dataset\n",
            "[[492   2  93]\n",
            " [ 49  16 188]\n",
            " [ 73   8 371]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82       587\n",
            "           1       0.62      0.06      0.11       253\n",
            "           2       0.57      0.82      0.67       452\n",
            "\n",
            "    accuracy                           0.68      1292\n",
            "   macro avg       0.66      0.57      0.54      1292\n",
            "weighted avg       0.68      0.68      0.63      1292\n",
            "\n",
            "Testing on the Libanon Dataset\n",
            "[[ 502    0   24]\n",
            " [   2    0    5]\n",
            " [   8    0 1299]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97       526\n",
            "           1       0.00      0.00      0.00         7\n",
            "           2       0.98      0.99      0.99      1307\n",
            "\n",
            "    accuracy                           0.98      1840\n",
            "   macro avg       0.65      0.65      0.65      1840\n",
            "weighted avg       0.98      0.98      0.98      1840\n",
            "\n",
            "Testing on the Egyptian Dataset\n",
            "[[96  0  0]\n",
            " [ 0 42  0]\n",
            " [ 1  0 81]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99        96\n",
            "           1       1.00      1.00      1.00        42\n",
            "           2       1.00      0.99      0.99        82\n",
            "\n",
            "    accuracy                           1.00       220\n",
            "   macro avg       1.00      1.00      1.00       220\n",
            "weighted avg       1.00      1.00      1.00       220\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YxXQ409tiq5i"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
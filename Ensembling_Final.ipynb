{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeIk2jBxF6bujbxKzEMqS2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImenMasmoudiEm/CCKInternshipProject/blob/main/Ensembling_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mIp4l4Y0_79",
        "outputId": "19d45db5-8ba8-42c1-c8d5-548f9963521e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H85ybNJ_yLfe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import os \n",
        "os.chdir('/content/drive/MyDrive/All/Projects/Ing Internship/Data')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading Data\n"
      ],
      "metadata": {
        "id": "klpWdEse1MPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tunnel=pd.read_excel(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/final-dataset.xlsx\")\n",
        "Tun=pd.read_excel(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/dataset1and2.xlsx\")\n",
        "Egiptien=pd.read_excel(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/dataset3and4.xlsx\")\n",
        "Libanon=pd.read_excel(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/D5.xlsx\")"
      ],
      "metadata": {
        "id": "kjdUVLTX1Oog"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For the Tunnal Dataset\n",
        "Tunnel['classe']=Tunnel['classe'].replace(\"hate\", int(2))\n",
        "Tunnel['classe']=Tunnel['classe'].replace(\"normal\", int(0)) \n",
        "Tunnel['classe']=Tunnel['classe'].replace(\"abusive\", int(1))\n",
        "\n",
        "\n",
        "TunnelS = Tunnel['commentaire']\n",
        "TunnelL = Tunnel['classe']\n",
        "\n",
        "TunnalL = [int(i) for i in TunnelL]"
      ],
      "metadata": {
        "id": "Cn6s5VUy1pCa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_size=int(len(TunnelS)*0.9) \n",
        "\n",
        "training_sentences = TunnelS[0:training_size]\n",
        "TunnelS = TunnelS[training_size:]\n",
        "TunnelL = TunnelL[training_size:]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=3000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "#For Tunnel Dataset\n",
        "TunnelS = tokenizer.texts_to_sequences(TunnelS)\n",
        "TunnelS207 = pad_sequences(TunnelS, maxlen=207, padding='post', truncating='post')\n",
        "TunnelS388 = pad_sequences(TunnelS, maxlen=388, padding='post', truncating='post')\n",
        "\n",
        "#For Tunisian Dataset\n",
        "\"\"\"training_size=int(len(TunS)*0.8) \n",
        "\n",
        "TunS = TunS[training_size:]\n",
        "TunL = TunL[training_size:]\n",
        "\n",
        "TunS = tokenizer.texts_to_sequences(TunS)\n",
        "TunS207 = pad_sequences(TunS, maxlen=207, padding='post', truncating='post')\n",
        "TunS388 = pad_sequences(TunS, maxlen=388, padding='post', truncating='post')\n",
        "\n",
        "#For Egiption Dataset\n",
        "training_size=int(len(EgyS)*0.8) \n",
        "\n",
        "EgyS = EgyS[training_size:]\n",
        "EgyL = EgyL[training_size:]\n",
        "\n",
        "EgyS = tokenizer.texts_to_sequences(EgyS)\n",
        "EgyS207 = pad_sequences(EgyS, maxlen=207, padding='post', truncating='post')\n",
        "EgyS388 = pad_sequences(EgyS, maxlen=388, padding='post', truncating='post')\n",
        "\n",
        "#For Libanon Dataset\n",
        "training_size=int(len(TunS)*0.8) \n",
        "\n",
        "TunS = TunS[training_size:]\n",
        "TunL = TunL[training_size:]\n",
        "\n",
        "TunS = tokenizer.texts_to_sequences(TunS)\n",
        "TunS207 = pad_sequences(TunS, maxlen=207, padding='post', truncating='post')\n",
        "TunS388 = pad_sequences(TunS, maxlen=388, padding='post', truncating='post')\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "JWuvFt7Y2g2L",
        "outputId": "7667c2e3-3f91-4b09-fff4-24b2562432ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"training_size=int(len(TunS)*0.8) \\n\\nTunS = TunS[training_size:]\\nTunL = TunL[training_size:]\\n\\nTunS = tokenizer.texts_to_sequences(TunS)\\nTunS207 = pad_sequences(TunS, maxlen=207, padding='post', truncating='post')\\nTunS388 = pad_sequences(TunS, maxlen=388, padding='post', truncating='post')\\n\\n#For Egiption Dataset\\ntraining_size=int(len(EgyS)*0.8) \\n\\nEgyS = EgyS[training_size:]\\nEgyL = EgyL[training_size:]\\n\\nEgyS = tokenizer.texts_to_sequences(EgyS)\\nEgyS207 = pad_sequences(EgyS, maxlen=207, padding='post', truncating='post')\\nEgyS388 = pad_sequences(EgyS, maxlen=388, padding='post', truncating='post')\\n\\n#For Libanon Dataset\\ntraining_size=int(len(TunS)*0.8) \\n\\nTunS = TunS[training_size:]\\nTunL = TunL[training_size:]\\n\\nTunS = tokenizer.texts_to_sequences(TunS)\\nTunS207 = pad_sequences(TunS, maxlen=207, padding='post', truncating='post')\\nTunS388 = pad_sequences(TunS, maxlen=388, padding='post', truncating='post')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the models\n",
        "M1B09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Balanced 09/Model1Balanced09.h5\")\n",
        "M1U09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Unbalanced 09/Model1Unbalanced09.h5\")\n",
        "M2B09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Balanced 09/Model2Balanced09.h5\")\n",
        "M2U09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Unbalanced 09/Model2Unbalanced09.h5\")\n",
        "M3B09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Balanced 09/Model3Balanced09.h5\")\n",
        "M3U09=tf.keras.models.load_model(\"/content/drive/MyDrive/All/Projects/Ing Internship/Data/Unbalanced 09/Model3Unbalanced09.h5\")"
      ],
      "metadata": {
        "id": "W3gFxkmB5pou"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fcount(c,L):\n",
        "  r=0\n",
        "  for i in L:\n",
        "    if i==c:\n",
        "      r+=1\n",
        "  return r\n",
        "\n",
        "def MaxV(L1,L2,L3,L4,L5,L6):\n",
        "  L=np.zeros(len(L1))\n",
        "  for i in range(len(L1)):\n",
        "    if (fcount(2,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])>fcount(0,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])) and (fcount(2,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])>fcount(1,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])):\n",
        "      L[i]=2\n",
        "    elif (fcount(1,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])>fcount(0,[L1[i],L2[i],L3[i],L4[i],L5[i],L6[i]])):\n",
        "      L[i]=1\n",
        "    else:\n",
        "      L[i]=0\n",
        "  return L"
      ],
      "metadata": {
        "id": "ZyCSUrO258Ll"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions for Tunnel\n",
        "TunnelP1=M1B09.predict(TunnelS207)\n",
        "TunnelP1=np.argmax(TunnelP1, axis=1).astype(int)\n",
        "\n",
        "TunnelP2=M1U09.predict(TunnelS388)\n",
        "TunnelP2=np.argmax(TunnelP2, axis=1).astype(int)\n",
        "\n",
        "TunnelP3=M2B09.predict(TunnelS207)\n",
        "TunnelP3=np.argmax(TunnelP3, axis=1).astype(int)\n",
        "\n",
        "TunnelP4=M2U09.predict(TunnelS388)\n",
        "TunnelP4=np.argmax(TunnelP4, axis=1).astype(int)\n",
        "\n",
        "TunnelP5=M3B09.predict(TunnelS207)\n",
        "TunnelP5=np.argmax(TunnelP5, axis=1).astype(int)\n",
        "\n",
        "TunnelP6=M3U09.predict(TunnelS388)\n",
        "TunnelP6=np.argmax(TunnelP6, axis=1).astype(int)"
      ],
      "metadata": {
        "id": "1hU12_YP3aUi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TunnelP=MaxV(TunnelP1,TunnelP2,TunnelP3,TunnelP4,TunnelP5,TunnelP6)\n",
        "\n",
        "cm = confusion_matrix(TunnelL, TunnelP)\n",
        "print(cm)\n",
        "print(classification_report(TunnelL, TunnelP, labels=[0,1,2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_olf9YI-zL4",
        "outputId": "bdeb50e5-d6ca-426c-8ece-0084a6e80f6a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1256    3    6]\n",
            " [ 135  268   11]\n",
            " [ 187  100  338]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.99      0.88      1265\n",
            "           1       0.72      0.65      0.68       414\n",
            "           2       0.95      0.54      0.69       625\n",
            "\n",
            "    accuracy                           0.81      2304\n",
            "   macro avg       0.82      0.73      0.75      2304\n",
            "weighted avg       0.83      0.81      0.79      2304\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TunnelL=np.asarray(TunnelL)\n",
        "TunnelP=np.asarray(TunnelP)\n",
        "X=[]\n",
        "for i in range(len(TunnelL)):\n",
        "  #print([TunnelP1[i],TunnelP2[i],TunnelP3[i],TunnelP4[i],TunnelP5[i],TunnelP6[i]],\" the true value is: \", TunnelL[i], \"P= \", TunnelP[i])\n",
        "  X.append([TunnelP1[i],TunnelP2[i],TunnelP3[i],TunnelP4[i],TunnelP5[i],TunnelP6[i]])"
      ],
      "metadata": {
        "id": "cOIfAkZzjXIQ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.asarray(X)"
      ],
      "metadata": {
        "id": "hwlNLFSVMwW0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TunnelL=np.asarray(TunnelL)"
      ],
      "metadata": {
        "id": "u0vMl0zzj02m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEModel = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(6,)),\n",
        "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(24, activation='selu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "FEModel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EFktnE7km3P",
        "outputId": "fb3fe014-700b-485d-fba5-4b8f79788578"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 6)                 0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               896       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 24)                3096      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 3)                 75        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,067\n",
            "Trainable params: 4,067\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FEModel.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
        "history = FEModel.fit(X, TunnelL, epochs=50, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adwrCC1Ok853",
        "outputId": "22471cde-1f2e-4ad0-d2fc-0e2e213d35b7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.7418 - accuracy: 0.7079\n",
            "Epoch 2/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.8832\n",
            "Epoch 3/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.3439 - accuracy: 0.9575\n",
            "Epoch 4/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.9562\n",
            "Epoch 5/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2985 - accuracy: 0.9588\n",
            "Epoch 6/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2931 - accuracy: 0.9588\n",
            "Epoch 7/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.9588\n",
            "Epoch 8/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.9588\n",
            "Epoch 9/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.9588\n",
            "Epoch 10/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.9588\n",
            "Epoch 11/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.9588\n",
            "Epoch 12/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2523 - accuracy: 0.9588\n",
            "Epoch 13/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.9588\n",
            "Epoch 14/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2337 - accuracy: 0.9588\n",
            "Epoch 15/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.9588\n",
            "Epoch 16/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2253 - accuracy: 0.9588\n",
            "Epoch 17/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.9588\n",
            "Epoch 18/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2133 - accuracy: 0.9588\n",
            "Epoch 19/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9588\n",
            "Epoch 20/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2125 - accuracy: 0.9588\n",
            "Epoch 21/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2072 - accuracy: 0.9588\n",
            "Epoch 22/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2080 - accuracy: 0.9588\n",
            "Epoch 23/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2055 - accuracy: 0.9588\n",
            "Epoch 24/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9588\n",
            "Epoch 25/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9588\n",
            "Epoch 26/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2042 - accuracy: 0.9588\n",
            "Epoch 27/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9588\n",
            "Epoch 28/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2024 - accuracy: 0.9588\n",
            "Epoch 29/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9588\n",
            "Epoch 30/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9588\n",
            "Epoch 31/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2026 - accuracy: 0.9588\n",
            "Epoch 32/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2031 - accuracy: 0.9588\n",
            "Epoch 33/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9588\n",
            "Epoch 34/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2073 - accuracy: 0.9588\n",
            "Epoch 35/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2045 - accuracy: 0.9588\n",
            "Epoch 36/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2032 - accuracy: 0.9588\n",
            "Epoch 37/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2031 - accuracy: 0.9588\n",
            "Epoch 38/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2009 - accuracy: 0.9588\n",
            "Epoch 39/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2062 - accuracy: 0.9588\n",
            "Epoch 40/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2058 - accuracy: 0.9588\n",
            "Epoch 41/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9588\n",
            "Epoch 42/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2034 - accuracy: 0.9588\n",
            "Epoch 43/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1995 - accuracy: 0.9588\n",
            "Epoch 44/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2064 - accuracy: 0.9588\n",
            "Epoch 45/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9588\n",
            "Epoch 46/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9588\n",
            "Epoch 47/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2011 - accuracy: 0.9588\n",
            "Epoch 48/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2008 - accuracy: 0.9588\n",
            "Epoch 49/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.9588\n",
            "Epoch 50/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TunnelFP=FEModel.predict(X)"
      ],
      "metadata": {
        "id": "MRnFhH1YOPz2"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TunnelFP=np.argmax(TunnelFP, axis=1).astype(int)\n",
        "cm = confusion_matrix(TunnelL, TunnelFP)\n",
        "print(cm)\n",
        "print(classification_report(TunnelL, TunnelFP, labels=[0,1,2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sp_G1OwOej9",
        "outputId": "e61fbaf5-c138-419b-a7b6-a187eb4470c2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1254    2    9]\n",
            " [  15  375   24]\n",
            " [  33   12  580]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98      1265\n",
            "           1       0.96      0.91      0.93       414\n",
            "           2       0.95      0.93      0.94       625\n",
            "\n",
            "    accuracy                           0.96      2304\n",
            "   macro avg       0.96      0.94      0.95      2304\n",
            "weighted avg       0.96      0.96      0.96      2304\n",
            "\n"
          ]
        }
      ]
    }
  ]
}